{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './utils/')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set number of iterations\n",
    "'''\n",
    "n_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:2')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        super().__init__()\n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        \n",
    "        # settings\n",
    "        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('lambda_1', self.lambda_1)\n",
    "        self.dnn.register_parameter('lambda_2', self.lambda_2)\n",
    "        \n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=n_iter, \n",
    "            max_eval=n_iter, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "        self.iter = 0\n",
    "        \n",
    "        self.history = []\n",
    "\n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = torch.exp(self.lambda_2)\n",
    "        u = self.net_u(x, t)\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        return f\n",
    "    \n",
    "    def loss_func_data_driven(self):\n",
    "        u_pred = self.net_u(self.x, self.t)\n",
    "        loss = torch.mean((self.u - u_pred) ** 2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.history.append(loss.item())\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Loss: %e, l1: %.5f, l2: %.5f' % \n",
    "                (\n",
    "                    loss.item(), \n",
    "                    self.lambda_1.item(), \n",
    "                    torch.exp(self.lambda_2.detach()).item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def loss_func_physics(self):\n",
    "        f_pred = self.net_f(self.x, self.t)\n",
    "        loss = torch.mean(f_pred ** 2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.history.append(loss.item())\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Loss: %e, l1: %.5f, l2: %.5f' % \n",
    "                (\n",
    "                    loss.item(), \n",
    "                    self.lambda_1.item(), \n",
    "                    torch.exp(self.lambda_2.detach()).item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def train_data_driven(self, nIter):\n",
    "        self.dnn.train()\n",
    "        history = []\n",
    "        for epoch in range(nIter):\n",
    "            u_pred = self.net_u(self.x, self.t)\n",
    "            loss = torch.mean((self.u - u_pred) ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.history.append(loss.item())\n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(), \n",
    "                        self.lambda_1.item(), \n",
    "                        torch.exp(self.lambda_2).item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func_data_driven)\n",
    "    \n",
    "    def train_data_physics(self, nIter):\n",
    "        # self.dnn.train()\n",
    "        history = []\n",
    "        for epoch in range(nIter):\n",
    "            f_pred = self.net_f(self.x, self.t)\n",
    "            loss = torch.mean(f_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.history.append(loss.item())\n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(), \n",
    "                        self.lambda_1.item(), \n",
    "                        torch.exp(self.lambda_2).item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func_physics)\n",
    "        \n",
    "    def predict_data_driven(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f\n",
    "    \n",
    "    def predict_physics(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "\n",
    "N_u = 2000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Non-noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStage I\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stage I\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 4.195e-01, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 100, Loss: 8.047e-02, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 200, Loss: 2.811e-02, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 300, Loss: 1.313e-02, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 400, Loss: 9.945e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 500, Loss: 6.246e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 600, Loss: 3.737e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 700, Loss: 2.672e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 800, Loss: 2.025e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 900, Loss: 1.623e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1000, Loss: 1.309e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1100, Loss: 1.106e-03, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1200, Loss: 8.714e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1300, Loss: 7.222e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1400, Loss: 6.142e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1500, Loss: 5.264e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1600, Loss: 4.600e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1700, Loss: 3.952e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1800, Loss: 3.493e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 1900, Loss: 2.948e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2000, Loss: 2.567e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2100, Loss: 2.205e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2200, Loss: 1.900e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2300, Loss: 1.667e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2400, Loss: 1.461e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2500, Loss: 1.277e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2600, Loss: 1.146e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2700, Loss: 1.021e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2800, Loss: 9.279e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 2900, Loss: 8.619e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3000, Loss: 7.909e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3100, Loss: 8.984e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3200, Loss: 6.550e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3300, Loss: 7.501e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3400, Loss: 6.109e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3500, Loss: 5.567e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3600, Loss: 5.145e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3700, Loss: 4.806e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3800, Loss: 4.987e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 3900, Loss: 4.731e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4000, Loss: 1.519e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4100, Loss: 4.020e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4200, Loss: 3.637e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4300, Loss: 3.989e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4400, Loss: 6.328e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4500, Loss: 3.418e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4600, Loss: 4.250e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4700, Loss: 3.148e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4800, Loss: 8.866e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 4900, Loss: 2.969e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5000, Loss: 2.613e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5100, Loss: 2.638e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5200, Loss: 2.482e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5300, Loss: 2.830e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5400, Loss: 2.312e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5500, Loss: 1.234e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5600, Loss: 3.756e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5700, Loss: 2.241e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5800, Loss: 2.405e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 5900, Loss: 2.283e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6000, Loss: 1.940e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6100, Loss: 2.006e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6200, Loss: 2.078e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6300, Loss: 8.694e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6400, Loss: 2.248e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6500, Loss: 1.951e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6600, Loss: 1.770e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6700, Loss: 3.512e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6800, Loss: 1.455e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 6900, Loss: 1.455e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7000, Loss: 2.759e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7100, Loss: 1.545e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7200, Loss: 3.146e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7300, Loss: 1.506e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7400, Loss: 1.280e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7500, Loss: 1.355e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7600, Loss: 2.972e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7700, Loss: 1.194e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7800, Loss: 1.472e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 7900, Loss: 3.395e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8000, Loss: 1.263e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8100, Loss: 1.090e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8200, Loss: 1.330e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8300, Loss: 1.114e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8400, Loss: 6.459e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8500, Loss: 2.324e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8600, Loss: 1.027e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8700, Loss: 1.525e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8800, Loss: 2.572e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 8900, Loss: 1.253e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9000, Loss: 1.948e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9100, Loss: 1.399e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9200, Loss: 1.852e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9300, Loss: 1.275e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9400, Loss: 1.071e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9500, Loss: 2.714e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9600, Loss: 1.669e-04, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9700, Loss: 1.180e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9800, Loss: 3.874e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "It: 9900, Loss: 4.193e-05, Lambda_1: 0.000, Lambda_2: 0.002479\n",
      "CPU times: user 57.5 s, sys: 1.35 s, total: 58.9 s\n",
      "Wall time: 58.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "noise = 0.0            \n",
    "\n",
    "# create training set\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# training\n",
    "model_DD = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "model_DD.train_data_driven(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5c844359d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Default training: Without loss weighing\")\n",
    "plt.xlabel(\"BFGS iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(model_DD.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 7.492132e-03\n",
      "Error l1: 100.00000%\n",
      "Error l2: 22.12771%\n"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "u_pred, f_pred = model_DD.predict_data_driven(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "lambda_1_value = model_DD.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value = model_DD.lambda_2.detach().cpu().numpy()\n",
    "lambda_2_value = np.exp(lambda_2_value)\n",
    "\n",
    "error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
    "error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_DD.state_dict(), 'model_data_driven_10k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStage II\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stage II\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelphy = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "modelphy.load_state_dict(torch.load('model_data_driven_10k.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelphy.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "# modelphy.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 3.455e+00, Lambda_1: 0.001, Lambda_2: 0.002476\n",
      "It: 100, Loss: 3.818e-02, Lambda_1: 0.019, Lambda_2: 0.002431\n",
      "It: 200, Loss: 1.451e-02, Lambda_1: 0.020, Lambda_2: 0.002426\n",
      "It: 300, Loss: 6.950e-03, Lambda_1: 0.020, Lambda_2: 0.002422\n",
      "It: 400, Loss: 3.906e-03, Lambda_1: 0.020, Lambda_2: 0.002420\n",
      "It: 500, Loss: 2.638e-03, Lambda_1: 0.020, Lambda_2: 0.002419\n",
      "It: 600, Loss: 2.006e-03, Lambda_1: 0.020, Lambda_2: 0.002418\n",
      "It: 700, Loss: 1.603e-03, Lambda_1: 0.021, Lambda_2: 0.002417\n",
      "It: 800, Loss: 1.310e-03, Lambda_1: 0.021, Lambda_2: 0.002417\n",
      "It: 900, Loss: 1.085e-03, Lambda_1: 0.021, Lambda_2: 0.002416\n",
      "It: 1000, Loss: 9.050e-04, Lambda_1: 0.021, Lambda_2: 0.002415\n",
      "It: 1100, Loss: 7.592e-04, Lambda_1: 0.021, Lambda_2: 0.002415\n",
      "It: 1200, Loss: 6.393e-04, Lambda_1: 0.021, Lambda_2: 0.002414\n",
      "It: 1300, Loss: 5.400e-04, Lambda_1: 0.021, Lambda_2: 0.002414\n",
      "It: 1400, Loss: 4.574e-04, Lambda_1: 0.021, Lambda_2: 0.002413\n",
      "It: 1500, Loss: 3.885e-04, Lambda_1: 0.021, Lambda_2: 0.002413\n",
      "It: 1600, Loss: 3.308e-04, Lambda_1: 0.021, Lambda_2: 0.002413\n",
      "It: 1700, Loss: 2.823e-04, Lambda_1: 0.021, Lambda_2: 0.002412\n",
      "It: 1800, Loss: 2.415e-04, Lambda_1: 0.021, Lambda_2: 0.002412\n",
      "It: 1900, Loss: 2.069e-04, Lambda_1: 0.021, Lambda_2: 0.002412\n",
      "It: 2000, Loss: 1.776e-04, Lambda_1: 0.021, Lambda_2: 0.002412\n",
      "It: 2100, Loss: 1.526e-04, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2200, Loss: 1.312e-04, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2300, Loss: 1.131e-04, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2400, Loss: 9.755e-05, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2500, Loss: 8.436e-05, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2600, Loss: 7.315e-05, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2700, Loss: 6.362e-05, Lambda_1: 0.021, Lambda_2: 0.002411\n",
      "It: 2800, Loss: 5.553e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 2900, Loss: 4.865e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3000, Loss: 4.281e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3100, Loss: 3.784e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3200, Loss: 3.360e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3300, Loss: 2.999e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3400, Loss: 2.689e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3500, Loss: 2.422e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3600, Loss: 2.192e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3700, Loss: 1.992e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3800, Loss: 1.816e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 3900, Loss: 1.663e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4000, Loss: 1.527e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4100, Loss: 1.406e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4200, Loss: 1.298e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4300, Loss: 1.202e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4400, Loss: 1.115e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4500, Loss: 1.038e-05, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4600, Loss: 9.671e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4700, Loss: 9.032e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4800, Loss: 8.451e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 4900, Loss: 7.919e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5000, Loss: 7.432e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5100, Loss: 6.985e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5200, Loss: 6.572e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5300, Loss: 6.190e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5400, Loss: 5.835e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5500, Loss: 5.506e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5600, Loss: 5.199e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5700, Loss: 4.912e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5800, Loss: 4.644e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 5900, Loss: 4.394e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6000, Loss: 4.159e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6100, Loss: 3.939e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6200, Loss: 3.733e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6300, Loss: 3.540e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6400, Loss: 3.358e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6500, Loss: 3.188e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6600, Loss: 3.029e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6700, Loss: 2.880e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6800, Loss: 2.739e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 6900, Loss: 2.608e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7000, Loss: 2.484e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7100, Loss: 2.368e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7200, Loss: 2.259e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7300, Loss: 2.156e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7400, Loss: 2.059e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7500, Loss: 1.967e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7600, Loss: 1.880e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7700, Loss: 1.798e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7800, Loss: 1.720e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 7900, Loss: 1.645e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8000, Loss: 1.574e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8100, Loss: 1.507e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8200, Loss: 1.442e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8300, Loss: 1.380e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8400, Loss: 1.321e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8500, Loss: 1.267e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8600, Loss: 1.445e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8700, Loss: 1.162e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8800, Loss: 2.899e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 8900, Loss: 1.072e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 9000, Loss: 1.027e-06, Lambda_1: 0.021, Lambda_2: 0.002410\n",
      "It: 9100, Loss: 1.007e-06, Lambda_1: 0.021, Lambda_2: 0.002409\n",
      "It: 9200, Loss: 9.487e-07, Lambda_1: 0.021, Lambda_2: 0.002409\n",
      "It: 9300, Loss: 4.671e-05, Lambda_1: 0.021, Lambda_2: 0.002409\n",
      "It: 9400, Loss: 8.779e-07, Lambda_1: 0.021, Lambda_2: 0.002409\n",
      "It: 9500, Loss: 8.467e-07, Lambda_1: 0.021, Lambda_2: 0.002409\n",
      "It: 9600, Loss: 8.177e-07, Lambda_1: 0.021, Lambda_2: 0.002408\n",
      "It: 9700, Loss: 7.778e-07, Lambda_1: 0.021, Lambda_2: 0.002408\n",
      "It: 9800, Loss: 1.190e-06, Lambda_1: 0.021, Lambda_2: 0.002408\n",
      "It: 9900, Loss: 7.217e-07, Lambda_1: 0.021, Lambda_2: 0.002408\n",
      "CPU times: user 2min 49s, sys: 1.23 s, total: 2min 50s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "noise = 0.0            \n",
    "\n",
    "# create training set\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# training\n",
    "modelphy.train_data_physics(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 1.041137e+00\n",
      "Error l1: 97.85362%\n",
      "Error l2: 24.36492%\n"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "u_pred, f_pred = modelphy.predict_data_driven(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "lambda_1_value = modelphy.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value = modelphy.lambda_2.detach().cpu().numpy()\n",
    "lambda_2_value = np.exp(lambda_2_value)\n",
    "\n",
    "error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
    "error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
